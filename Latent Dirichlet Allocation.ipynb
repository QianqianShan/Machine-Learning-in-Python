{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LDA introduction with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop List {'and', 'to', 'of', 'the', 'for', 'in', 'a'}\n",
      "Text = \n",
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
      " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'management', 'system'],\n",
      " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
      " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
      " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
      " ['intersection', 'graph', 'paths', 'trees'],\n",
      " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "f = open('Data-and-Output/22.LDA_test.txt')\n",
    "stop_list = set('for a of the and to in'.split())\n",
    "print(\"Stop List\", stop_list)\n",
    "# texts = [line.strip().split() for line in f]\n",
    "# print(texts)\n",
    "\n",
    "# put words not in the stop list in word list for each line\n",
    "texts = [[word for word in line.strip().lower().split() if word not in stop_list] for line in f]\n",
    "print ('Text = ')\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "[(0, 0.4301019571350565), (1, 0.4301019571350565), (2, 0.4301019571350565), (3, 0.2944198962221451), (4, 0.2944198962221451), (5, 0.2944198962221451), (6, 0.4301019571350565)]\n",
      "[(3, 0.3726494271826947), (7, 0.5443832091958983), (8, 0.27219160459794917), (9, 0.3726494271826947), (10, 0.3726494271826947), (11, 0.27219160459794917), (12, 0.3726494271826947)]\n",
      "[(5, 0.438482464916089), (8, 0.32027755044706185), (11, 0.32027755044706185), (13, 0.438482464916089), (14, 0.6405551008941237)]\n",
      "[(4, 0.3449874408519962), (8, 0.5039733231394895), (13, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]\n",
      "[(9, 0.30055933182961736), (10, 0.30055933182961736), (11, 0.21953536176370683), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]\n",
      "[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]\n",
      "[(25, 0.31622776601683794), (26, 0.31622776601683794), (27, 0.6324555320336759), (28, 0.6324555320336759)]\n",
      "[(25, 0.20466057569885868), (26, 0.20466057569885868), (29, 0.40932115139771735), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.2801947048062438)]\n",
      "[(12, 0.6282580468670046), (26, 0.45889394536615247), (34, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "V = len(dictionary)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "\n",
    "print ('TF-IDF:')\n",
    "for c in corpus_tfidf:\n",
    "    print (c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSI Model:\n",
      "[[(0, 0.34057117986841845), (1, -0.20602251622679627)],\n",
      " [(0, 0.69330400021715777), (1, 0.0072327583903876308)],\n",
      " [(0, 0.59026076703897246), (1, -0.35260469490855778)],\n",
      " [(0, 0.5214901821825132), (1, -0.33887976154055321)],\n",
      " [(0, 0.39533193176354536), (1, -0.059192853366601052)],\n",
      " [(0, 0.036353173528493515), (1, 0.18146550208818935)],\n",
      " [(0, 0.1470901232877902), (1, 0.49432948127822413)],\n",
      " [(0, 0.21407117317565358), (1, 0.640645666445395)],\n",
      " [(0, 0.40066568318170881), (1, 0.64131082990940003)]]\n"
     ]
    }
   ],
   "source": [
    "# LSI\n",
    "print ('\\nLSI Model:')\n",
    "lsi = models.LsiModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "topic_result = [a for a in lsi[corpus_tfidf]]\n",
    "pprint(topic_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI Topics:\n",
      "[(0,\n",
      "  '0.400*\"system\" + 0.318*\"survey\" + 0.290*\"user\" + 0.274*\"eps\" + '\n",
      "  '0.236*\"management\"'),\n",
      " (1,\n",
      "  '0.421*\"minors\" + 0.420*\"graph\" + 0.293*\"survey\" + 0.239*\"trees\" + '\n",
      "  '0.226*\"intersection\"')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ('LSI Topics:')\n",
    "pprint(lsi.print_topics(num_topics=2, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:\n",
      "[array([ 1.        ,  0.85017949,  0.99998462,  0.99948108,  0.92283762,\n",
      "       -0.33944285, -0.25207737, -0.21974573,  0.01438824], dtype=float32),\n",
      " array([ 0.85017949,  1.        ,  0.85309052,  0.83277911,  0.98737705,\n",
      "        0.20664607,  0.29518002,  0.32680073,  0.53867108], dtype=float32),\n",
      " array([ 0.99998462,  0.85309052,  1.        ,  0.99928677,  0.92496276,\n",
      "       -0.33421329, -0.24669875, -0.21432398,  0.01994151], dtype=float32),\n",
      " array([ 0.99948108,  0.83277911,  0.99928677,  1.        ,  0.90995121,\n",
      "       -0.3695657 , -0.28311783, -0.25105581, -0.0178274 ], dtype=float32),\n",
      " array([ 0.92283762,  0.98737705,  0.92496276,  0.90995121,  1.        ,\n",
      "        0.04906874,  0.14012395,  0.1729846 ,  0.39842743], dtype=float32),\n",
      " array([-0.33944285,  0.20664607, -0.33421329, -0.3695657 ,  0.04906874,\n",
      "        1.        ,  0.99581701,  0.99222624,  0.93564534], dtype=float32),\n",
      " array([-0.25207737,  0.29518002, -0.24669875, -0.28311783,  0.14012395,\n",
      "        0.99581701,  1.        ,  0.99944651,  0.96397996], dtype=float32),\n",
      " array([-0.21974573,  0.32680073, -0.21432398, -0.25105581,  0.1729846 ,\n",
      "        0.99222624,  0.99944651,  1.        ,  0.97229445], dtype=float32),\n",
      " array([ 0.01438824,  0.53867108,  0.01994151, -0.0178274 ,  0.39842743,\n",
      "        0.93564534,  0.96397996,  0.97229445,  1.        ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "similarity = similarities.MatrixSimilarity(lsi[corpus_tfidf])   # similarities.Similarity()\n",
    "print ('Similarity:')\n",
    "pprint(list(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Model:\n",
      "Document-Topic:\n",
      "\n",
      "[[(0, 0.30895863934234108), (1, 0.69104136065765898)],\n",
      " [(0, 0.29290268764942679), (1, 0.70709731235057327)],\n",
      " [(0, 0.68281219746677557), (1, 0.31718780253322443)],\n",
      " [(0, 0.39309811890975499), (1, 0.60690188109024501)],\n",
      " [(0, 0.71874716393273841), (1, 0.28125283606726148)],\n",
      " [(0, 0.4700878573572746), (1, 0.52991214264272546)],\n",
      " [(0, 0.2607668293175166), (1, 0.7392331706824834)],\n",
      " [(0, 0.34356255351665133), (1, 0.65643744648334867)],\n",
      " [(0, 0.30128658180272488), (1, 0.69871341819727517)]]\n"
     ]
    }
   ],
   "source": [
    "print ('\\nLDA Model:')\n",
    "num_topics = 2\n",
    "lda = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary,\n",
    "                    alpha='auto', eta='auto', minimum_probability=0.001)\n",
    "doc_topic = [doc_t for doc_t in lda[corpus_tfidf]]\n",
    "print ('Document-Topic:\\n')\n",
    "pprint(doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.30887851082408768), (1, 0.69112148917591232)]\n",
      "[(0, 0.29298891316066378), (1, 0.70701108683933622)]\n",
      "[(0, 0.68283931229294526), (1, 0.31716068770705474)]\n",
      "[(0, 0.39308287121750096), (1, 0.60691712878249915)]\n",
      "[(0, 0.71873407905906206), (1, 0.28126592094093789)]\n",
      "[(0, 0.47140919419008703), (1, 0.52859080580991291)]\n",
      "[(0, 0.26077543407291986), (1, 0.73922456592708019)]\n",
      "[(0, 0.34364866576534603), (1, 0.65635133423465397)]\n",
      "[(0, 0.30128599418534885), (1, 0.69871400581465104)]\n",
      "Topic 0\n",
      "[('system', 0.038959625985374939),\n",
      " ('management', 0.037822747437386645),\n",
      " ('eps', 0.036913584081606714),\n",
      " ('user', 0.036870163016899091),\n",
      " ('interface', 0.036187892029062009),\n",
      " ('minors', 0.034296477939576762),\n",
      " ('response', 0.031935855826525486),\n",
      " ('graph', 0.031481777666608889),\n",
      " ('time', 0.03142137430744333),\n",
      " ('measurement', 0.030726341900593736)]\n",
      "Topic 1\n",
      "[('survey', 0.043780206189694602),\n",
      " ('graph', 0.041371621807866799),\n",
      " ('system', 0.03961375301285943),\n",
      " ('minors', 0.036632477687497784),\n",
      " ('trees', 0.036298614813193686),\n",
      " ('paths', 0.034553858422203348),\n",
      " ('computer', 0.034134297270210862),\n",
      " ('intersection', 0.033975095907527723),\n",
      " ('opinion', 0.03152647221029703),\n",
      " ('human', 0.030643401628550584)]\n",
      "Similarity:\n",
      "[array([ 1.        ,  0.99961877,  0.75477934,  0.98808008,  0.71269375,\n",
      "        0.95295846,  0.99669546,  0.99808097,  0.9999131 ], dtype=float32),\n",
      " array([ 0.99961877,  1.        ,  0.73637944,  0.98345292,  0.69305366,\n",
      "        0.94422626,  0.99855822,  0.99599069,  0.99989587], dtype=float32),\n",
      " array([ 0.75477934,  0.73637944,  1.        ,  0.84676433,  0.99807948,\n",
      "        0.91810113,  0.69900072,  0.79395068,  0.74606574], dtype=float32),\n",
      " array([ 0.98808008,  0.98345292,  0.84676433,  1.        ,  0.81218415,\n",
      "        0.9882589 ,  0.97231048,  0.99571627,  0.98596472], dtype=float32),\n",
      " array([ 0.71269375,  0.69305366,  0.99807948,  0.81218415,  1.        ,\n",
      "        0.89178538,  0.65335852,  0.75476307,  0.70338398], dtype=float32),\n",
      " array([ 0.95295846,  0.94422626,  0.91810113,  0.9882589 ,  0.89178538,\n",
      "        1.        ,  0.92518878,  0.9698984 ,  0.94887972], dtype=float32),\n",
      " array([ 0.99669546,  0.99855822,  0.69900072,  0.97231048,  0.65335852,\n",
      "        0.92518878,  0.99999994,  0.98975283,  0.99767971], dtype=float32),\n",
      " array([ 0.99808097,  0.99599069,  0.79395068,  0.99571627,  0.75476307,\n",
      "        0.9698984 ,  0.98975283,  0.99999994,  0.99717784], dtype=float32),\n",
      " array([ 0.9999131 ,  0.99989587,  0.74606574,  0.98596472,  0.70338398,\n",
      "        0.94887972,  0.99767971,  0.99717784,  1.        ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for doc_topic in lda.get_document_topics(corpus_tfidf):\n",
    "    print (doc_topic)\n",
    "for topic_id in range(num_topics):\n",
    "    print ('Topic', topic_id)\n",
    "    # pprint(lda.get_topic_terms(topicid=topic_id))\n",
    "    pprint(lda.show_topic(topic_id))\n",
    "similarity = similarities.MatrixSimilarity(lda[corpus_tfidf])\n",
    "print ('Similarity:')\n",
    "pprint(list(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USE WITH CARE--\n",
      "HDA Model:\n",
      "[[(0, 0.14124363539490895),\n",
      "  (1, 0.70279978617999272),\n",
      "  (2, 0.039419969747645986),\n",
      "  (3, 0.029721359766011611),\n",
      "  (4, 0.022184556885793311),\n",
      "  (5, 0.016642918105538022),\n",
      "  (6, 0.012320848040085111)],\n",
      " [(0, 0.78993202493667791),\n",
      "  (1, 0.052977525671265929),\n",
      "  (2, 0.039759636712338293),\n",
      "  (3, 0.029926095205808211),\n",
      "  (4, 0.022335156778908911),\n",
      "  (5, 0.016755956463171469),\n",
      "  (6, 0.012404504831033751)],\n",
      " [(0, 0.084416228707431484),\n",
      "  (1, 0.73726637508697324),\n",
      "  (2, 0.045299775868464197),\n",
      "  (3, 0.03395068625525359),\n",
      "  (4, 0.025317037604487242),\n",
      "  (5, 0.018991230257945153),\n",
      "  (6, 0.014059272799183287),\n",
      "  (7, 0.010491734391670452)],\n",
      " [(0, 0.76443375145749992),\n",
      "  (1, 0.060021069544757215),\n",
      "  (2, 0.044377394905282253),\n",
      "  (3, 0.033455097028795525),\n",
      "  (4, 0.024972136911451569),\n",
      "  (5, 0.018731286850493412),\n",
      "  (6, 0.013866863813666443),\n",
      "  (7, 0.010348149146191095)],\n",
      " [(0, 0.78605560407410668),\n",
      "  (1, 0.056260380266803148),\n",
      "  (2, 0.040165549659906541),\n",
      "  (3, 0.030054919275414566),\n",
      "  (4, 0.02234984844599289),\n",
      "  (5, 0.016767309805085909),\n",
      "  (6, 0.012412922179054126)],\n",
      " [(0, 0.76360401110779552),\n",
      "  (1, 0.059823773770886199),\n",
      "  (2, 0.044615705212789979),\n",
      "  (3, 0.033662829769689304),\n",
      "  (4, 0.025117220487598766),\n",
      "  (5, 0.018843526035952528),\n",
      "  (6, 0.013949966001691976),\n",
      "  (7, 0.010410164244300149)],\n",
      " [(0, 0.74025649031135143),\n",
      "  (1, 0.065805144614019484),\n",
      "  (2, 0.04899634969596036),\n",
      "  (3, 0.03696433593435549),\n",
      "  (4, 0.027591743320603972),\n",
      "  (5, 0.020700019278252341),\n",
      "  (6, 0.015324342992356152),\n",
      "  (7, 0.011435793287037756)],\n",
      " [(0, 0.11179432085766286),\n",
      "  (1, 0.73770140341434642),\n",
      "  (2, 0.038087964639090845),\n",
      "  (3, 0.028678825525652742),\n",
      "  (4, 0.021398080505472976),\n",
      "  (5, 0.016052908023147565),\n",
      "  (6, 0.011884045631885639)],\n",
      " [(0, 0.72265397755608685),\n",
      "  (1, 0.070165769994833885),\n",
      "  (2, 0.052484958013491928),\n",
      "  (3, 0.039481242956208827),\n",
      "  (4, 0.029441530909853332),\n",
      "  (5, 0.02208711788241054),\n",
      "  (6, 0.016351209920028167),\n",
      "  (7, 0.012202092867258093)]]\n",
      "HDA Topics:\n",
      "[(0, '0.115*trees + 0.090*time + 0.084*computer + 0.063*error + 0.063*generation'), (1, '0.148*quasi + 0.094*human + 0.083*interface + 0.069*lab + 0.055*response')]\n"
     ]
    }
   ],
   "source": [
    "hda = models.HdpModel(corpus_tfidf, id2word=dictionary)\n",
    "topic_result = [a for a in hda[corpus_tfidf]]\n",
    "print ('\\n\\nUSE WITH CARE--\\nHDA Model:')\n",
    "pprint(topic_result)\n",
    "print ('HDA Topics:')\n",
    "print (hda.print_topics(num_topics=2, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint\n",
    "import time\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "def load_stopword():\n",
    "    f_stop = open('Data-and-Output/22.stopword.txt')\n",
    "    sw = [line.strip() for line in f_stop]\n",
    "    f_stop.close()\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化停止词列表 --\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa1 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9fc0c8a180e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'初始化停止词列表 --'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stopword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'开始读入语料数据 -- '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-bb7834eae9fd>\u001b[0m in \u001b[0;36mload_stopword\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_stopword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mf_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data-and-Output/22.stopword.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mf_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-bb7834eae9fd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_stopword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mf_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data-and-Output/22.stopword.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mf_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa1 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "print ('初始化停止词列表 --')\n",
    "t_start = time.time()\n",
    "stop_words = load_stopword()\n",
    "\n",
    "print ('开始读入语料数据 -- ')\n",
    "f = open('Data-and-Output/22.news.dat')    #22.LDA_test.txt\n",
    "texts = [[word for word in line.strip().lower().split() if word not in stop_words] for line in f]\n",
    "# texts = [line.strip().split() for line in f]\n",
    "print ('读入语料数据完成，用时%.3f秒' % (time.time() - t_start))\n",
    "f.close()\n",
    "M = len(texts)\n",
    "print ('文本数目：%d个' % M)\n",
    "# pprint(texts)\n",
    "\n",
    "\n",
    "print ('正在建立词典 --')\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "V = len(dictionary)\n",
    "print ('正在计算文本向量 --')\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print ('正在计算文档TF-IDF --')\n",
    "t_start = time.time()\n",
    "corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "print ('建立文档TF-IDF完成，用时%.3f秒' % (time.time() - t_start))\n",
    "print ('LDA模型拟合推断 --')\n",
    "num_topics = 30\n",
    "t_start = time.time()\n",
    "lda = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary,\n",
    "                        alpha=0.01, eta=0.01, minimum_probability=0.001,\n",
    "                        update_every = 1, chunksize = 100, passes = 1)\n",
    "print ('LDA模型完成，训练时间为\\t%.3f秒' % (time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 所有文档的主题\n",
    "# doc_topic = [a for a in lda[corpus_tfidf]]\n",
    "# print 'Document-Topic:\\n'\n",
    "# pprint(doc_topic)\n",
    "\n",
    "# 随机打印某10个文档的主题\n",
    " num_show_topic = 10  # 每个文档显示前几个主题\n",
    "print ('10个文档的主题分布：')\n",
    "doc_topics = lda.get_document_topics(corpus_tfidf)  # 所有文档的主题分布\n",
    "idx = np.arange(M)\n",
    "np.random.shuffle(idx)\n",
    "idx = idx[:10]\n",
    "for i in idx:\n",
    "    topic = np.array(doc_topics[i])\n",
    "    topic_distribute = np.array(topic[:, 1])\n",
    "    # print topic_distribute\n",
    "    topic_idx = topic_distribute.argsort()[:-num_show_topic-1:-1]\n",
    "    print ('第%d个文档的前%d个主题：' % (i, num_show_topic)), topic_idx\n",
    "    print topic_distribute[topic_idx]\n",
    "num_show_term = 7   # 每个主题显示几个词\n",
    "print ('每个主题的词分布：')\n",
    "for topic_id in range(num_topics):\n",
    "    print ('主题#%d：\\t' % topic_id)\n",
    "    term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "    term_distribute = term_distribute_all[:num_show_term]\n",
    "    term_distribute = np.array(term_distribute)\n",
    "    term_id = term_distribute[:, 0].astype(np.int)\n",
    "    print ('词：\\t',)\n",
    "    for t in term_id:\n",
    "        print (dictionary.id2token[t])\n",
    "    print\n",
    "    # print '\\n概率：\\t', term_distribute[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import lda\n",
    "import lda.datasets \n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lda' has no attribute 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b114a884f88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# document-term matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_reuters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type(X): {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape: {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lda' has no attribute 'datasets'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # document-term matrix\n",
    "    X = lda.datasets.load_reuters()\n",
    "    print(\"type(X): {}\".format(type(X)))\n",
    "    print(\"shape: {}\\n\".format(X.shape))\n",
    "    print(X[:10, :10])\n",
    "\n",
    "    # the vocab\n",
    "    vocab = lda.datasets.load_reuters_vocab()\n",
    "    print(\"type(vocab): {}\".format(type(vocab)))\n",
    "    print(\"len(vocab): {}\\n\".format(len(vocab)))\n",
    "    print(vocab[:10])\n",
    "\n",
    "    # titles for each story\n",
    "    titles = lda.datasets.load_reuters_titles()\n",
    "    print(\"type(titles): {}\".format(type(titles)))\n",
    "    print(\"len(titles): {}\\n\".format(len(titles)))\n",
    "    pprint(titles[:10])\n",
    "\n",
    "    print ('LDA start ----')\n",
    "    topic_num = 20\n",
    "    model = lda.LDA(n_topics=topic_num, n_iter=500, random_state=1)\n",
    "    model.fit(X)\n",
    "\n",
    "    # topic-word\n",
    "    topic_word = model.topic_word_\n",
    "    print(\"type(topic_word): {}\".format(type(topic_word)))\n",
    "    print(\"shape: {}\".format(topic_word.shape))\n",
    "    print(vocab[:5])\n",
    "    print(topic_word[:, :5])\n",
    "\n",
    "    # Print Topic distribution\n",
    "    n = 7\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n + 1):-1]\n",
    "        print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))\n",
    "\n",
    "    # Document - topic\n",
    "    doc_topic = model.doc_topic_\n",
    "    print(\"type(doc_topic): {}\".format(type(doc_topic)))\n",
    "    print(\"shape: {}\".format(doc_topic.shape))\n",
    "    for i in range(10):\n",
    "        topic_most_pr = doc_topic[i].argmax()\n",
    "        print(u\"文档: {} 主题: {} value: {}\".format(i, topic_most_pr, doc_topic[i][topic_most_pr]))\n",
    "\n",
    "    mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "    mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    # Topic - word\n",
    "    plt.figure(figsize=(8, 9))\n",
    "    # f, ax = plt.subplots(5, 1, sharex=True)\n",
    "    for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "        ax = plt.subplot(5, 1, i+1)\n",
    "        ax.plot(topic_word[k, :], 'r-')\n",
    "        ax.set_xlim(-50, 4350)   # [0,4258]\n",
    "        ax.set_ylim(0, 0.08)\n",
    "        ax.set_ylabel(u\"概率\")\n",
    "        ax.set_title(u\"主题 {}\".format(k))\n",
    "    plt.xlabel(u\"词\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(u'主题的词分布', fontsize=18)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "    # Document - Topic\n",
    "    plt.figure(figsize=(8, 9))\n",
    "    # f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "    for i, k in enumerate([1, 3, 4, 8, 9]):\n",
    "        ax = plt.subplot(5, 1, i+1)\n",
    "        ax.stem(doc_topic[k, :], linefmt='g-', markerfmt='ro')\n",
    "        ax.set_xlim(-1, topic_num+1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_ylabel(u\"概率\")\n",
    "        ax.set_title(u\"文档 {}\".format(k))\n",
    "    plt.xlabel(u\"主题\", fontsize=14)\n",
    "    plt.suptitle(u'文档的主题分布', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
